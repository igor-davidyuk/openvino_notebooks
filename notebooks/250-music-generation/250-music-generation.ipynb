{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2990976f-be06-4068-bf49-56d39a3f93a8",
   "metadata": {},
   "source": [
    "# Controllable Music Generation with MusicGen and OpenVINO\n",
    "\n",
    "MusicGen is a single-stage auto-regressive Transformer model capable of generating high-quality music samples conditioned on text descriptions or audio prompts. The text prompt is passed to a text encoder model (T5) to obtain a sequence of hidden-state representations. These hidden states are fed to MusicGen, which predicts discrete audio tokens (audio codes). Finally, audio tokens are then decoded using an audio compression model (EnCodec) to recover the audio waveform.\n",
    "\n",
    "![pipeline](https://)\n",
    "\n",
    "[The MusicGen model](https://arxiv.org/abs/2306.05284) does not require a self-supervised semantic representation of the text/audio prompts; it operates over several streams of compressed discrete music representation with efficient token interleaving patterns, thus eliminating the need to cascade multiple models to predict a set of codebooks (e.g. hierarchically or upsampling). Unlike prior models addressing music generation, it is able to generate all the codebooks in a single forward pass.\n",
    "\n",
    "In this tutorial, we consider how to run the MusicGen model using OpenVINO.\n",
    "\n",
    "We will use a model implementation from the [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) library. \n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "\n",
    "- Install prerequisites\n",
    "- Download the MusicGen Small model from a public source using the [Hugging Face Hub](https://huggingface.co/models?sort=downloads&search=facebook%2Fmusicgen-).\n",
    "- Run the text-conditioned music generation pipeline\n",
    "- Convert three models backing the MusicGen pipeline\n",
    "- Run the music generation pipeline again using OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e64a4-62a8-4de6-b1c1-a8f41199ac07",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1463e7-68f4-40e2-a151-4d6d7678f1e0",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d29d27-919b-407b-846d-f025b5e99878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U --quiet pip\n",
    "!pip install -q --pre openvino\n",
    "!pip install -q torch onnx gradio\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git datasets[audio]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2e282-19f1-47c4-bbac-03177db00243",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8494fc4-78f6-4f43-a4d3-f2ec18210355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Audio\n",
    "from openvino import Core, convert_model, PartialShape, save_model, Type\n",
    "import torch\n",
    "from torch.jit import TracerWarning\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n",
    "\n",
    "# Ignore tracing warnings\n",
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523545c2-3e07-466e-bce8-85152ed2f1f4",
   "metadata": {},
   "source": [
    "## MusicGen in HF Transformers\r\n",
    "\r\n",
    "To work with MusicGen by Meta AI, we will use [Hugging Face Transformers package](https://github.com/huggingface/transformers). Transformers package exposes the `MusicgenForConditionalGeneration` class, simplifying the model instantiation and weights loading. The code below demonstrates how to create a `MusicgenForConditionalGeneration` and generate a text-conditioned music sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d1128f-d892-49dc-8525-f757c34cc33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\", torchscript=True, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8f6270-e745-4adb-b65d-c1d8dc44d7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 tokens will be generated\n",
      "Sampling rate is 32000 Hz\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "sample_length = 10  # seconds\n",
    "n_tokens = sample_length * model.config.audio_encoder.frame_rate\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "print(n_tokens, 'tokens will be generated')\n",
    "print('Sampling rate is', sampling_rate, 'Hz')\n",
    "\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83117816-3ec2-43a0-8fcd-1eb09c6e21e9",
   "metadata": {},
   "source": [
    "### Original Pipeline Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3101955-6c20-4fc6-b675-1237260972bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/musicgen-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m      4\u001b[0m     text\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m80s pop track with bassy drums and synth\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m audio_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m Audio(audio_values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), rate\u001b[38;5;241m=\u001b[39msampling_rate)\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:2435\u001b[0m, in \u001b[0;36mMusicgenForConditionalGeneration.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   2427\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2428\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2429\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2430\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2431\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2432\u001b[0m     )\n\u001b[1;32m   2434\u001b[0m     \u001b[38;5;66;03m# 12. run sample\u001b[39;00m\n\u001b[0;32m-> 2435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2436\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2444\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2445\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot incompatible mode for generation, should be one of greedy or sampling.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure that beam search is de-activated by setting `num_beams=1` and `num_beam_groups=1`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2452\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/transformers/generation/utils.py:2723\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2720\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2722\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2723\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2724\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2726\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2728\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2731\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:1917\u001b[0m, in \u001b[0;36mMusicgenForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, input_values, padding_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1914\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m audio_codes[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mnum_codebooks, seq_len)\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1917\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[21], line 39\u001b[0m, in \u001b[0;36mMusicGenWrapper.forward\u001b[0;34m(self, input_ids, encoder_hidden_states, encoder_attention_mask, past_key_values, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmusic_gen_lm\n\u001b[1;32m     37\u001b[0m     arguments \u001b[38;5;241m=\u001b[39m (input_ids, encoder_hidden_states, encoder_attention_mask, \u001b[38;5;241m*\u001b[39mpast_key_values)\n\u001b[0;32m---> 39\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutputWithCrossAttentions(\n\u001b[1;32m     41\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(output[model\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]]),\n\u001b[1;32m     42\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([output[model\u001b[38;5;241m.\u001b[39moutputs[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m97\u001b[39m)]),\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/openvino/runtime/ie_api.py:384\u001b[0m, in \u001b[0;36mCompiledModel.__call__\u001b[0;34m(self, inputs, share_inputs, share_outputs, shared_memory)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_request \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_infer_request()\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshare_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_deprecated_memory_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshare_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/openvino/runtime/ie_api.py:143\u001b[0m, in \u001b[0;36mInferRequest.infer\u001b[0;34m(self, inputs, share_inputs, share_outputs, shared_memory)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     70\u001b[0m     inputs: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     shared_memory: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OVDict:\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Infers specified input(s) in synchronous mode.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    Blocks all methods of InferRequest while request is running.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    :rtype: OVDict\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OVDict(\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_data_dispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_shared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_deprecated_memory_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_outputs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "audio_values = model.generate(**inputs.to(device), do_sample=True, guidance_scale=3, max_new_tokens=n_tokens)\n",
    "\n",
    "Audio(audio_values[0].cpu().numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f4db8-759d-47f8-912c-b11d7ba9b632",
   "metadata": {},
   "source": [
    "## Convert models to OpenVINO Intermediate representation (IR) format\r\n",
    "Model conversion API enables direct conversion of PyTorch models. We will utilize the `openvino.ovc.convert_model` method to acquire OpenVINO IR versions of the models. The method requires a model object and example input for model tracing. Under the hood,the  converter will use the PyTorch JIT compiler, to build a frozen model graph.\r\n",
    "\r\n",
    "The pipeline consists of three important parts:\r\n",
    "\r\n",
    " - The T5 Text Encohat translates user prompts to vectors in the latent space that the next model - MusicGen decoder can utilize.\r\n",
    " - A MusicGen Language Model that auto-regressively generates audio tokens (codes).\r\n",
    " - EnCodec model (we will use only the decoder partof), it is used decode the audio waveform from the audio tokens predicted by the MusicGen LM\r\n",
    "\r\n",
    "Let us convert each model step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d43b6-5a9f-41ae-aad5-c66e832ddd84",
   "metadata": {},
   "source": [
    "### 0. Set Up Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867f36ab-6cd6-4f50-a23c-e5069091cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"./models\")\n",
    "t5_ir_path = models_dir / \"t5.xml\"\n",
    "musicgen_0_ir_path = models_dir / \"mg_0.xml\"\n",
    "musicgen_ir_path = models_dir / \"mg.xml\"\n",
    "audio_decoder_onnx_path = models_dir / 'encodec.onnx'\n",
    "audio_decoder_ir_path = models_dir / \"encodec.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04569e5-3042-41e2-95e7-c75aed6855ff",
   "metadata": {},
   "source": [
    "### 1. Convert Text Encoder\r\n",
    "\r\n",
    "The text encoder is responsible for converting the input prompt, such as \"90s rock song with loud guitars and heavy drums\" into an embedding space that can be fed to the next model. Typically, it is a transformer-based encoder that maps a sequence of input tokens to a sequence of text embeddings.\r\n",
    "\r\n",
    "The input for the text encoder consists of a tensor `input_ids`, which contains token indices from the text processed by the token,izer and `attention_mask` that we will ignore as we will process one prompt at a time and it will just consist of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dadad3c-06ce-43e5-b059-b16238f3963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not t5_ir_path.exists():\n",
    "    t5_ov = convert_model(model.text_encoder, example_input={'input_ids': inputs['input_ids']})\n",
    "\n",
    "    save_model(t5_ov, t5_ir_path)\n",
    "    del t5_ov\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98019b03-8c03-47e4-b311-8bb6af45b70c",
   "metadata": {},
   "source": [
    "### 2. Convert MusicGen Language Model\n",
    "\n",
    "This model is the central part of the whole pipeline, it takes the embedded text representation and generates audio codes that can be then decoded into actual music. The model outputs several streams of audio codes - tokens sampled from the pre-trained codebooks representing music efficiently with a lower framerate. The model employs innovative codes intervaling strategy, that makes single-stage generation possible.\n",
    "\n",
    "On the 0th generation step the model accepts `input_ids` representing the indices of audio codes, `encoder_hidden_states` and `encoder_attention_mask` that were provided by the text encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791f3259-ea4c-459a-b4b0-a19e61e79c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model config `torchscript` to True, so the model returns a tuple as output\n",
    "model.decoder.config.torchscript = True\n",
    "\n",
    "if not musicgen_0_ir_path.exists():\n",
    "    decoder_input = {\n",
    "        'input_ids': torch.ones(8, 1, dtype=torch.int64),\n",
    "        'encoder_hidden_states':  torch.ones(2, 12, 1024, dtype=torch.float32),\n",
    "        'encoder_attention_mask':  torch.ones(2, 12, dtype=torch.int64),\n",
    "    }\n",
    "    mg_ov_0_step = convert_model(model.decoder, example_input=decoder_input)\n",
    "\n",
    "    save_model(mg_ov_0_step, musicgen_0_ir_path)\n",
    "    del mg_ov_0_step\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1882240a-3300-4c72-9a6b-fe72fd6820b3",
   "metadata": {},
   "source": [
    "On further iterations, the model is also provided with a `past_key_values` argument that contains previous outputs of the attention block, it allows us to save on computations.\n",
    "But for us, it means that the signature of the model's `forward` method changed. Models in OpenVINO IR have frozen calculation graphs and do not allow optional arguments, that is why the MusicGen model must be converted a second time, with an increased number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd21b7d-2026-4781-849f-a7bff22e63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional argument to the example_input dict\n",
    "if not musicgen_ir_path.exists():\n",
    "    # Add `past_key_values` to the converted model signature  \n",
    "    decoder_input['past_key_values'] = tuple(\n",
    "        [(\n",
    "            torch.ones(2, 16, 1, 64, dtype=torch.float32),\n",
    "            torch.ones(2, 16, 1, 64, dtype=torch.float32),\n",
    "            torch.ones(2, 16, 12, 64, dtype=torch.float32),\n",
    "            torch.ones(2, 16, 12, 64, dtype=torch.float32),\n",
    "        )]*24\n",
    "    )\n",
    "\n",
    "    mg_ov = convert_model(model.decoder, example_input=decoder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74671a-25e2-4369-b260-5768cbbe1209",
   "metadata": {},
   "source": [
    "Moreover, the `past_key_values` argument is passed as `Tuple[Tuple[torch.tensor]]` which is a hard call for the converter. The converter flattens these tuples, but have hard time identifying the correct shapes for the tensors. The code below goes over raveled `past_key_values`-related model inputs and sets the correct shape and type for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89be419c-4e46-48c1-8b64-e0970e5ab444",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not musicgen_ir_path.exists():\n",
    "    for input in mg_ov.inputs[3:]:\n",
    "        input.get_node().set_partial_shape(PartialShape([-1, 16, -1, 64]))\n",
    "        input.get_node().set_element_type(Type.f32)\n",
    "    \n",
    "    mg_ov.validate_nodes_and_infer_types()\n",
    "\n",
    "    save_model(mg_ov, musicgen_ir_path)\n",
    "    del mg_ov\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93a8a2-312b-46c3-85b9-df0cd96feeb6",
   "metadata": {},
   "source": [
    "### 3. Convert Audio Decoder\n",
    "\n",
    "The audio decoder which is a part of the EnCodec model is used to recover the audio waveform from the audio tokens predicted by the MusicGen decoder. To learn more about the model please refer to the corresponding [OpenVINO example](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/234-encodec-audio-compression).\n",
    "\n",
    "The audio decoder computation graph contains an LSTM sub-network which currently should be converted through ONNX.\n",
    "To do this we create a wrapper class with its `forward()` method calling `encodec.decode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ca233ad-d8a2-46ca-ba99-a054647df626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idavidyu/.virtualenvs/test/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:4476: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not audio_decoder_onnx_path.exists():\n",
    "    class AudioDecoder(torch.nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "    \n",
    "        def forward(self, output_ids):\n",
    "            return self.model.decode(output_ids, [None])\n",
    "\n",
    "    audio_decoder_input = {'output_ids': torch.ones(1, 1, 4, 253, dtype=torch.int64),}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            model=AudioDecoder(model.audio_encoder), \n",
    "            args=audio_decoder_input,\n",
    "            f=audio_decoder_onnx_path,\n",
    "            input_names=['output_ids',],\n",
    "            output_names=['decoded_audio'],\n",
    "            dynamic_axes={'output_ids': {3: 'sequence_length'},\n",
    "                          'decoded_audio': {2: 'audio_values'}\n",
    "                         }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "212f404d-ca14-4b44-840f-cdc73b52f20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 161920)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input = {'output_ids': torch.ones(1, 1, 4, 500, dtype=torch.int64),}\n",
    "core.compile_model(str(audio_decoder_onnx_path))(list(encoder_input.values()))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21655571-ba05-41d3-9a0c-160efac4cf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 320000])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.audio_encoder.decode(torch.ones(1, 1, 4, 500, dtype=torch.int64), [None]).audio_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bbd6612-aa59-469f-a352-9bdd043e5a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "161920 / 253 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e45623e0-ee26-4d9f-883f-510f0bb9fd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "320000 / 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5945458e-2f76-4ab9-89d6-172785b3e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can convert the model to OpenVINO IR\n",
    "if not audio_decoder_ir_path.exists():\n",
    "    audio_decoder_ov =  convert_model(str(audio_decoder_onnx_path))\n",
    "\n",
    "    save_model(audio_decoder_ov, audio_decoder_ir_path)\n",
    "    del audio_decoder_ov\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1172fd5-ebc8-4cec-a42e-1a69ec684226",
   "metadata": {},
   "source": [
    "## Embeddingthe converted models into the original pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0231dea8-24dd-46bc-b5d6-399e72b3c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaee5ead-f0ce-40be-b678-99518e1b9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder_ir, config):\n",
    "        super().__init__()\n",
    "        self.encoder = core.compile_model(encoder_ir)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        last_hidden_state = self.encoder(input_ids)[self.encoder.outputs[0]]\n",
    "        last_hidden_state = torch.tensor(last_hidden_state)\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=last_hidden_state)\n",
    "\n",
    "\n",
    "class MusicGenWrapper(torch.nn.Module):\n",
    "    def __init__(self, music_gen_lm_0_ir, music_gen_lm_ir, config, num_codebooks, build_delay_pattern_mask,\n",
    "                 apply_delay_pattern_mask):\n",
    "        super().__init__()\n",
    "        self.music_gen_lm_0 = core.compile_model(music_gen_lm_0_ir)\n",
    "        self.music_gen_lm = core.compile_model(music_gen_lm_ir)\n",
    "        self.config = config\n",
    "        self.num_codebooks = num_codebooks\n",
    "        self.build_delay_pattern_mask = build_delay_pattern_mask\n",
    "        self.apply_delay_pattern_mask = apply_delay_pattern_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        encoder_hidden_states: torch.FloatTensor = None,\n",
    "        encoder_attention_mask: torch.LongTensor = None,\n",
    "        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if past_key_values is None:\n",
    "            model = self.music_gen_lm_0\n",
    "            arguments = (input_ids, encoder_hidden_states, encoder_attention_mask)\n",
    "        else:\n",
    "            model = self.music_gen_lm\n",
    "            arguments = (input_ids, encoder_hidden_states, encoder_attention_mask, *past_key_values)\n",
    "            \n",
    "        output = model(arguments)\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            logits = torch.tensor(output[model.outputs[0]]),\n",
    "            past_key_values = tuple([output[model.outputs[i]] for i in range(1, 97)]),\n",
    "        )\n",
    "\n",
    "class AudioDecoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, decoder_ir, config):\n",
    "        super().__init__()\n",
    "        self.decoder = core.compile_model(decoder_ir)\n",
    "        self.config = config\n",
    "        self.output_type = namedtuple(\"AudioDecoderOutput\", [\"audio_values\"])\n",
    "\n",
    "    def decode(self, output_ids, audio_scales):\n",
    "        output = self.decoder(output_ids)[self.decoder.outputs[0]]\n",
    "        return self.output_type(audio_values=torch.tensor(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a4ee8dd-1d6f-47c0-a7e2-42c8cb5ef714",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception from src/inference/src/infer_request.cpp:206:\nCheck 'inputs.size() == 1' failed at src/inference/src/infer_request.cpp:208:\nget_input_tensor() must be called on a function with exactly one parameter.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/musicgen-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m     23\u001b[0m     text\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m80s pop track with bassy drums and synth\u001b[39m\u001b[38;5;124m\"\u001b[39m,],\n\u001b[1;32m     24\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m audio_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m Audio(audio_values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), rate\u001b[38;5;241m=\u001b[39msampling_rate)\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/transformers/models/musicgen/modeling_musicgen.py:2474\u001b[0m, in \u001b[0;36mMusicgenForConditionalGeneration.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio_scales \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2472\u001b[0m     audio_scales \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m-> 2474\u001b[0m output_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_scales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_scales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate:\n\u001b[1;32m   2480\u001b[0m     outputs\u001b[38;5;241m.\u001b[39msequences \u001b[38;5;241m=\u001b[39m output_values\u001b[38;5;241m.\u001b[39maudio_values\n",
      "Cell \u001b[0;32mIn[21], line 53\u001b[0m, in \u001b[0;36mAudioDecoderWrapper.decode\u001b[0;34m(self, output_ids, audio_scales)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_ids, audio_scales):\n\u001b[0;32m---> 53\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_ids\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_type(audio_values\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(output))\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/openvino/runtime/ie_api.py:384\u001b[0m, in \u001b[0;36mCompiledModel.__call__\u001b[0;34m(self, inputs, share_inputs, share_outputs, shared_memory)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_request \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_infer_request()\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshare_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_deprecated_memory_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshare_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/openvino/runtime/ie_api.py:143\u001b[0m, in \u001b[0;36mInferRequest.infer\u001b[0;34m(self, inputs, share_inputs, share_outputs, shared_memory)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     70\u001b[0m     inputs: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     shared_memory: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OVDict:\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Infers specified input(s) in synchronous mode.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    Blocks all methods of InferRequest while request is running.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    :rtype: OVDict\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OVDict(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minfer(\u001b[43m_data_dispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_shared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_deprecated_memory_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, share_outputs\u001b[38;5;241m=\u001b[39mshare_outputs))\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/openvino/runtime/utils/data_helpers/data_dispatcher.py:354\u001b[0m, in \u001b[0;36m_data_dispatch\u001b[0;34m(request, inputs, is_shared)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_shared\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m is_shared \u001b[38;5;28;01melse\u001b[39;00m create_copied(inputs, request)\n",
      "File \u001b[0;32m/usr/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/openvino/runtime/utils/data_helpers/data_dispatcher.py:169\u001b[0m, in \u001b[0;36mcreate_shared\u001b[0;34m(inputs, request)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m     request\u001b[38;5;241m.\u001b[39m_inputs_data \u001b[38;5;241m=\u001b[39m normalize_arrays(inputs, is_shared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_shared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Error should be raised if type does not match any dispatchers\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible inputs of type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/openvino/runtime/utils/data_helpers/data_dispatcher.py:59\u001b[0m, in \u001b[0;36m_\u001b[0;34m(value, request, is_shared, key)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;129m@value_to_tensor\u001b[39m\u001b[38;5;241m.\u001b[39mregister(np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_\u001b[39m(\n\u001b[1;32m     54\u001b[0m     value: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     key: Optional[ValidKeys] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mget_request_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     tensor_type \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mget_element_type()\n\u001b[1;32m     61\u001b[0m     tensor_dtype \u001b[38;5;241m=\u001b[39m tensor_type\u001b[38;5;241m.\u001b[39mto_dtype()\n",
      "File \u001b[0;32m~/.virtualenvs/test/lib/python3.10/site-packages/openvino/runtime/utils/data_helpers/data_dispatcher.py:23\u001b[0m, in \u001b[0;36mget_request_tensor\u001b[0;34m(request, key)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_request_tensor\u001b[39m(\n\u001b[1;32m     19\u001b[0m     request: _InferRequestWrapper,\n\u001b[1;32m     20\u001b[0m     key: Optional[ValidKeys] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m request\u001b[38;5;241m.\u001b[39mget_input_tensor(key)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception from src/inference/src/infer_request.cpp:206:\nCheck 'inputs.size() == 1' failed at src/inference/src/infer_request.cpp:208:\nget_input_tensor() must be called on a function with exactly one parameter.\n\n"
     ]
    }
   ],
   "source": [
    "text_encode_ov = TextEncoderWrapper(t5_ir_path, model.text_encoder.config)\n",
    "musicgen_decoder_ov = MusicGenWrapper(\n",
    "    musicgen_0_ir_path,\n",
    "    musicgen_ir_path,\n",
    "    model.decoder.config,\n",
    "    model.decoder.num_codebooks,\n",
    "    model.decoder.build_delay_pattern_mask,\n",
    "    model.decoder.apply_delay_pattern_mask\n",
    ")\n",
    "audio_encoder_ov = AudioDecoderWrapper(audio_decoder_ir_path, model.audio_encoder.config)\n",
    "\n",
    "del model.text_encoder\n",
    "del model.decoder\n",
    "del model.audio_encoder\n",
    "gc.collect()\n",
    "\n",
    "model.text_encoder = text_encode_ov\n",
    "model.decoder = musicgen_decoder_ov\n",
    "model.audio_encoder = audio_encoder_ov\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth\",],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "audio_values = model.generate(**inputs.to(device), do_sample=True, guidance_scale=3, max_new_tokens=n_tokens)\n",
    "Audio(audio_values[0].cpu().numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c3d3c-3ccd-48a0-8682-3d854ebd15b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04059aa3-8549-4635-a51a-74c026f1d740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
