{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbbf681",
   "metadata": {},
   "source": [
    "# Image generation with DeepFloyd IF and OpenVINOâ„¢\n",
    "\n",
    "DeepFloyd IF is an advanced open-source text-to-image model that delivers remarkable photorealism and language comprehension. DeepFloyd IF consists of a frozen text encoder and three cascaded pixel diffusion modules: a base model that creates 64x64 px images based on text prompts and two super-resolution models, each designed to generate images with increasing resolution: 256x256 px and 1024x1024 px. All stages of the model employ a frozen text encoder, built on the T5 transformer, to derive text embeddings, which are then passed to a UNet architecture enhanced with cross-attention and attention pooling.\n",
    "\n",
    "![deepfloyd_if_scheme](https://github.com/deep-floyd/IF/raw/develop/pics/deepfloyd_if_scheme.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca7602",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e9c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set up requirements\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install deepfloyd_if==1.0.2rc0\n",
    "pip install xformers==0.0.16\n",
    "pip install git+https://github.com/openai/CLIP.git --no-deps\n",
    "pip install huggingface_hub\n",
    "pip install --upgrade diffusers accelerate transformers safetensors\n",
    "pip install openvino-dev==2023.0.0.dev20230407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.utils import pt_to_pil\n",
    "from openvino.runtime import Core, serialize\n",
    "from openvino.tools import mo\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "output_dtype = torch.float32\n",
    "compress_to_fp16 = False\n",
    "\n",
    "models_dir = Path('./models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "encoder_ir_path = models_dir / 'encoder_ir.xml'\n",
    "first_stage_unet_ir_path = models_dir / 'unet_ir_I_l.xml'\n",
    "second_stage_unet_ir_path = models_dir / 'unet_ir_II_m.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c37ae",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "In order to access IF checkpoints, users need to provide an authentication token. To generate a token, follow the link displayed in the cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75db9d49",
   "metadata": {},
   "source": [
    "## Stable Diffusion in Diffusers library\n",
    "To work with IF by DeepFloyd Lab, we will use Hugging Face Diffusers library. To experiment with diffusion models, Diffusers exposes the DiffusionPipeline. The code below demonstrates how to create a DiffusionPipeline using IF configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Downloading the model weights may take some time. The approximate total checkpoints size is 27GB.\n",
    "stage_1 = DiffusionPipeline.from_pretrained(\n",
    "    \"DeepFloyd/IF-I-L-v1.0\",\n",
    "    variant=\"fp32\",\n",
    "    torch_dtype=output_dtype\n",
    ")\n",
    "\n",
    "stage_2 = DiffusionPipeline.from_pretrained(\n",
    "    \"DeepFloyd/IF-II-M-v1.0\",\n",
    "    text_encoder=None,\n",
    "    variant=\"fp32\",\n",
    "    torch_dtype=output_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8cd8f",
   "metadata": {},
   "source": [
    "## Convert models to OpenVINO Intermediate representation (IR) format\n",
    "The OpenVINO Model Optimizer enables direct conversion of PyTorch models. We will utilize the mo.convert_model method to acquire OpenVINO IR versions of the models. This requires providing a model object, input data for model tracing, and other relevant parameters. The use_legacy_frontend=True parameter instructs the Model Optimizer to employ the ONNX model format as an intermediate step, as opposed to using the PyTorch JIT compiler, which is not optimal for our situation.\n",
    "\n",
    "The pipeline consists of three important parts:\n",
    "\n",
    " - A Text Encoder that translates user prompts to vectors in the latent space that the Diffusion model can understand.\n",
    " - A Stage 1 U-Net for step-by-step denoising latent image representation.\n",
    " - A Stage 2 U-Net that takes low resolution output from the previous step and the latent representations to upscale the resulting image.\n",
    " \n",
    "Let us convert each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This context manager will keep intermediate ONNX model weights out of the working directory.\n",
    "\n",
    "import contextlib\n",
    "import tempfile\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def temp_dir():\n",
    "    cwd = Path.cwd()\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "    os.chdir(temp_dir.name)\n",
    "    yield\n",
    "    os.chdir(cwd)\n",
    "    temp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d2557",
   "metadata": {},
   "source": [
    "## 1. Convert Text Encoder\n",
    "\n",
    "The text encoder is responsible for converting the input prompt, such as \"ultra close-up color photo portrait of rainbow owl with deer horns in the woods\" into an embedding space that can be fed to the next stage's U-Net. Typically, it is a transformer-based encoder that maps a sequence of input tokens to a sequence of text embeddings.\n",
    "\n",
    "The input for the text encoder consists of a tensor `input_ids`, which contains token indices from the text processed by the tokenizer and padded to the maximum length accepted by the model, and `attention_mask`, which marks relevant tokens with 1s and padded tokens with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617d3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not encoder_ir_path.exists():\n",
    "    # Define example inputs for model conversion\n",
    "    example_inputs = {\n",
    "        'input_ids': torch.ones((1, 77), dtype=torch.long),\n",
    "        'attention_mask': torch.ones((1, 77), dtype=torch.long)\n",
    "    }\n",
    "    \n",
    "    with temp_dir():\n",
    "        encoder_ir = mo.convert_model(\n",
    "            stage_1.text_encoder,\n",
    "            example_input=example_inputs,\n",
    "            input_shape=[[-1, 77], [-1, 77]],\n",
    "            compress_to_fp16=compress_to_fp16,\n",
    "            progress=True,\n",
    "            onnx_opset_version=14,\n",
    "            use_legacy_frontend=True\n",
    "        )\n",
    "\n",
    "    serialize(encoder_ir, encoder_ir_path)\n",
    "    del encoder_ir\n",
    "    \n",
    "del stage_1.text_encoder\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5214461",
   "metadata": {},
   "source": [
    "## Convert the first Pixel Diffusion module's UNet\n",
    "\n",
    "U-Net model gradually denoises latent image representation guided by text encoder hidden state.\n",
    "\n",
    "U-Net model has three inputs:\n",
    "\n",
    "`sample` - latent image sample from previous step. Generation process has not been started yet, so you will use random noise.\n",
    "`timestep` - current scheduler step.\n",
    "`encoder_hidden_state` - hidden state of text encoder.\n",
    "Model predicts the sample state for the next step.\n",
    "\n",
    "The first Diffusion module in the cascade generates 64x64 pixel low resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not first_stage_unet_ir_path.exists():\n",
    "    example_inputs = {\n",
    "        'sample': torch.rand((2, 3, 64, 64), device=device, dtype=output_dtype),\n",
    "        'timestep': torch.tensor([500], device=device, dtype=output_dtype),\n",
    "        'encoder_hidden_states': torch.rand((2, 77, 4096), device=device, dtype=output_dtype),\n",
    "    }\n",
    "\n",
    "    with temp_dir():\n",
    "        unet_1_ir = mo.convert_model(\n",
    "            stage_1.unet,\n",
    "            example_input=example_inputs,\n",
    "            compress_to_fp16=compress_to_fp16,\n",
    "            input_shape=[[-1, 3, -1, -1], [1], [-1, 77, 4096]],\n",
    "            progress=True,\n",
    "            onnx_opset_version=14,\n",
    "            use_legacy_frontend=True\n",
    "        )\n",
    "\n",
    "    serialize(unet_1_ir, first_stage_unet_ir_path)\n",
    "    \n",
    "    del unet_1_ir\n",
    "\n",
    "stage_1_config = stage_1.unet.config\n",
    "del stage_1.unet\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69845c5f",
   "metadata": {},
   "source": [
    "## Convert the second pixel diffusion module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20b312",
   "metadata": {},
   "source": [
    "The second Diffusion module in the cascade generates 256x256 pixel images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not second_stage_unet_ir_path.exists():\n",
    "    example_inputs = {\n",
    "        'sample': torch.rand((2, 6, 256, 256), device=device, dtype=output_dtype),\n",
    "        'timestep': torch.tensor([500], device=device, dtype=output_dtype),\n",
    "        'encoder_hidden_states': torch.rand((2, 77, 4096), device=device, dtype=output_dtype),\n",
    "        'class_labels': torch.tensor([250, 250])\n",
    "    }\n",
    "\n",
    "    with temp_dir():\n",
    "        unet_2_ir = mo.convert_model(\n",
    "            stage_2.unet,\n",
    "            example_input=example_inputs,\n",
    "            compress_to_fp16=compress_to_fp16,\n",
    "            input_shape=[[-1, 6, 256, 256], [1], [-1, 77, 4096], [-1]],\n",
    "            progress=True,\n",
    "            onnx_opset_version=14,\n",
    "            use_legacy_frontend=True\n",
    "        )\n",
    "\n",
    "    serialize(unet_2_ir, second_stage_unet_ir_path)\n",
    "    \n",
    "    del unet_2_ir\n",
    "    \n",
    "stage_2_config = stage_2.unet.config\n",
    "del stage_2.unet\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e578523",
   "metadata": {},
   "source": [
    "## Prepare Inference pipeline\n",
    "\n",
    "The original pipeline from the source repository will be reused in this example. In order to achieve this, adapter classes were created to enable OpenVINO models to replace Pytorch models and integrate seamlessly into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6887246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self, ir_path, dtype=output_dtype):\n",
    "        self.ir_path = ir_path \n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.encoder_openvino = core.compile_model(self.ir_path, \"CPU\")\n",
    "        try:\n",
    "            result = self.encoder_openvino(list(args) + list(kwargs.values()))\n",
    "            result_numpy = result[self.encoder_openvino.outputs[0]]\n",
    "        finally:\n",
    "            del self.encoder_openvino\n",
    "            gc.collect()\n",
    "        return [torch.tensor(result_numpy, dtype=self.dtype)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetFirstStage:\n",
    "    def __init__(self, unet_ir_path, config, dtype=output_dtype):\n",
    "        self.unet_openvino = core.compile_model(unet_ir_path, \"CPU\")\n",
    "        self.config = config\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        parameters = [*args, kwargs['encoder_hidden_states']]\n",
    "        parameters[1] = torch.tensor(parameters[1])\n",
    "        result = self.unet_openvino(parameters)\n",
    "        result_numpy = result[self.unet_openvino.outputs[0]]\n",
    "        class a:\n",
    "            sample = torch.tensor(result_numpy, dtype=self.dtype)\n",
    "        return a\n",
    "    \n",
    "class UnetSecondStage:\n",
    "    def __init__(self, unet_ir_path, config, dtype=output_dtype):\n",
    "        self.unet_openvino = core.compile_model(unet_ir_path, \"CPU\")\n",
    "        self.config = config\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        parameters = [*args, kwargs['encoder_hidden_states'], kwargs['class_labels']]\n",
    "        parameters[1] = torch.tensor(parameters[1])\n",
    "        result = self.unet_openvino(parameters)\n",
    "        result_numpy = result[self.unet_openvino.outputs[0]]\n",
    "        class a:\n",
    "            sample = torch.tensor(result_numpy, dtype=self.dtype)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd5641",
   "metadata": {},
   "source": [
    "## Run Text-to-Image generation\n",
    "\n",
    "Now, we can set a text prompt for image generation and execute the inference pipeline. Optionally, you can also modify the random generator seed for latent state initialization and adjust the number of images to be generated for the given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fc9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stage_1.text_encoder = TextEncoder(encoder_ir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prompt = 'uultra close-p color photo portrait of rainbow owl with deer horns in the woods'\n",
    "count = 1\n",
    "\n",
    "# text embeds\n",
    "prompt_embeds, negative_embeds = stage_1.encode_prompt(prompt, num_images_per_prompt=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f968dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stage_1.unet = UnetFirstStage(first_stage_unet_ir_path, stage_1_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ca973",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "generator = torch.manual_seed(142)\n",
    "\n",
    "image = stage_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\").images\n",
    "pt_to_pil(image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "stage_2.unet = UnetSecondStage(second_stage_unet_ir_path, stage_2_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image = stage_2(\n",
    "    image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\"\n",
    ").images\n",
    "for i, im in enumerate(pt_to_pil(image)):\n",
    "    im.save(f\"./if_stage_II_ov_{i}.png\")\n",
    "pt_to_pil(image)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
