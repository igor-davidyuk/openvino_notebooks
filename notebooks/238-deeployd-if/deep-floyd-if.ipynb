{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fbbf681",
   "metadata": {},
   "source": [
    "# Image generation with DeepFloyd IF and OpenVINOâ„¢\n",
    "\n",
    "DeepFloyd IF is an advanced open-source text-to-image model that delivers remarkable photorealism and language comprehension. DeepFloyd IF consists of a frozen text encoder and three cascaded pixel diffusion modules: a base model that creates 64x64 px images based on text prompts and two super-resolution models, each designed to generate images with increasing resolution: 256x256 px and 1024x1024 px. All stages of the model employ a frozen text encoder, built on the T5 transformer, to derive text embeddings, which are then passed to a UNet architecture enhanced with cross-attention and attention pooling.\n",
    "\n",
    "![deepfloyd_if_scheme](https://github.com/deep-floyd/IF/raw/develop/pics/deepfloyd_if_scheme.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca7602",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conda deactivate && conda remove -n if --all -y || 1\n",
    "# conda create -n if python=3.9 -y\n",
    "# conda activate if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699e9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up requirements\n",
    "\n",
    "# pip install --upgrade pip\n",
    "# pip install deepfloyd_if==1.0.2rc0\n",
    "# pip install xformers==0.0.16\n",
    "# pip install git+https://github.com/openai/CLIP.git --no-deps\n",
    "# pip install huggingface_hub\n",
    "# pip install --upgrade diffusers accelerate transformers safetensors\n",
    "# pip install openvino-dev==2023.0.0.dev20230407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7701f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCE_MEM_EFFICIENT_ATTN= 0 @UNET:QKVATTENTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 20:30:47.419592: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Memory efficient attention is not supported by ONNX\n",
    "os.environ['FORCE_MEM_EFFICIENT_ATTN'] = \"0\"\n",
    "\n",
    "from deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII\n",
    "from deepfloyd_if.modules.t5 import T5Embedder\n",
    "from deepfloyd_if.pipelines import dream\n",
    "from openvino.runtime import Core, serialize\n",
    "from openvino.tools import mo\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1c7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "encoder_ir_path = 'encoder_ir.xml'\n",
    "first_stage_unet_ir_path = './unet_ir.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c44d2557",
   "metadata": {},
   "source": [
    "## Convert text encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8f6fe68",
   "metadata": {},
   "source": [
    "### Initialize Pytorch model\n",
    "\n",
    "Downloading the model weights may take some time. Approximate checkpoint size is 20GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5746b289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idavidyu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1104: FutureWarning: The `force_filename` parameter is deprecated as a new caching system, which keeps the filenames as they are on the Hub, is now in place.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cfe1fecf8944e4adec09488ed8b529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 597 ms, sys: 6.72 s, total: 7.32 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t5 = T5Embedder(device=device, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a617d3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idavidyu/.local/lib/python3.10/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input_0\n",
      "  warnings.warn(\n",
      "/home/idavidyu/.local/lib/python3.10/site-packages/torch/onnx/utils.py:2040: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input_1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [....................] 100.00% doneCPU times: user 3min 10s, sys: 2min 1s, total: 5min 11s\n",
      "Wall time: 9min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Top memory consumption during this cell execution is 45GB\n",
    "\n",
    "# Define example inputs for model conversion\n",
    "example_inputs = {\n",
    "    'input_ids': torch.ones((1, 77), dtype=torch.long),\n",
    "    'attention_mask': torch.ones((1, 77), dtype=torch.long)\n",
    "}\n",
    "\n",
    "encoder_ir = mo.convert_model(\n",
    "    t5.model,\n",
    "    example_input=example_inputs,\n",
    "    input_shape=[[-1, 77], [-1, 77]],\n",
    "    compress_to_fp16=False,\n",
    "    progress=True,\n",
    "    onnx_opset_version=14,\n",
    "    use_legacy_frontend=True\n",
    ")\n",
    "\n",
    "serialize(encoder_ir, str(encoder_ir_path))\n",
    "\n",
    "del t5.model\n",
    "del encoder_ir\n",
    "gc.collect();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5214461",
   "metadata": {},
   "source": [
    "## Convert the first pixel diffusion module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71dc73",
   "metadata": {},
   "source": [
    "## Initialize Pytorch model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52dedf02",
   "metadata": {},
   "source": [
    "The first stage UNet requires conversion using a CUDA device because an operation in the model is not supported by the CPU backend library (\"cos_vml_cpu\" not implemented for 'Half').\n",
    "\n",
    "Alternatively, one can download the same model in fp32 precision using the Diffusers API. Here's the code for converting the model, although I've experienced issues with the conversion never completing:\n",
    "```\n",
    "from diffusers import DiffusionPipeline\n",
    "from openvino.tools import mo\n",
    "import torch\n",
    "\n",
    "stage_1 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-M-v1.0\", variant=\"fp32\", torch_dtype=torch.float32)\n",
    "\n",
    "example_inputs = {\n",
    "    'sample': torch.rand((2, 3, 64, 64), device=device, dtype=torch.float32),\n",
    "    'timestep': torch.rand((2), device=device, dtype=torch.float32),\n",
    "    'encoder_hidden_states': torch.rand((2, 77, 4096), device=device, dtype=torch.float32),\n",
    "}\n",
    "\n",
    "unet_1_ir = mo.convert_model(\n",
    "    stage_1.unet,\n",
    "    example_input=example_inputs,\n",
    "    compress_to_fp16=False,\n",
    "    input_shape=[[-1, 3, -1, -1], [-1,], [-1, 77, 4096]],\n",
    "    progress=True,\n",
    "    onnx_opset_version=14,\n",
    "    use_legacy_frontend=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c17f09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.29 s, sys: 717 ms, total: 2 s\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# # \"cos_vml_cpu\" not implemented for 'Half'\n",
    "device = 'cpu'\n",
    "device='cuda'\n",
    "if_I = IFStageI(\"IF-I-M-v1.0\", device=device)\n",
    "# if_I.model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8adf1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "example_inputs = {\n",
    "    'x': torch.rand((2, 3, 64, 64), device=device, dtype=torch.float32),\n",
    "    'timesteps': torch.rand((2), device=device, dtype=torch.float32),\n",
    "    'text_emb': torch.rand((2, 77, 4096), device=device, dtype=torch.float32),\n",
    "}\n",
    "\n",
    "unet_1_ir = mo.convert_model(\n",
    "    if_I.model,\n",
    "    example_input=example_inputs,\n",
    "    compress_to_fp16=False,\n",
    "    input_shape=[[-1, 3, -1, -1], [-1,], [-1, 77, 4096]],\n",
    "    progress=True,\n",
    "    onnx_opset_version=14,\n",
    "    use_legacy_frontend=True\n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "if_I.device = device\n",
    "\n",
    "\n",
    "serialize(unet_1_ir, first_stage_unet_ir_path)\n",
    "del unet_1_ir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e578523",
   "metadata": {},
   "source": [
    "## Prepare Inference pipeline\n",
    "\n",
    "The original pipeline from the source repository will be reused in this example. In order to achieve this, adapter classes were created to enable OpenVINO models to replace Pytorch models and integrate seamlessly into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86c94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6887246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self, encoder_ir_path, dtype=torch.float16):\n",
    "        self.encoder_ir_path = encoder_ir_path\n",
    "#         self.encoder_openvino = core.compile_model(encoder_ir_path, \"CPU\")\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        print(\"ENCODER CALL\")\n",
    "        self.encoder_openvino = core.compile_model(encoder_ir_path, \"CPU\")\n",
    "        \n",
    "        result = self.encoder_openvino(*args, list(kwargs.values()))\n",
    "        result_numpy = result[self.encoder_openvino.outputs[0]]\n",
    "        \n",
    "        del self.encoder_openvino\n",
    "        gc.collect()\n",
    "        return {'last_hidden_state': torch.tensor(result_numpy, dtype=self.dtype)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af6e4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetFirstStage:\n",
    "    def __init__(self, unet_ir_path, dtype=torch.float16):\n",
    "        self.unet_openvino = core.compile_model(unet_ir_path, \"CPU\")\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        parameters = [*args, kwargs['text_emb']]\n",
    "        # [t.cpu() for t in parameters]\n",
    "        result = self.unet_openvino(parameters)\n",
    "        result_numpy = result[self.unet_openvino.outputs[0]]\n",
    "        return torch.tensor(result_numpy, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37b9d6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.83 s, sys: 1.87 s, total: 4.7 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t5.device = 'cpu'\n",
    "if_I.device = 'cpu'\n",
    "\n",
    "del if_I.model\n",
    "del t5.model\n",
    "gc.collect();\n",
    "\n",
    "t5.model = TextEncoder(encoder_ir_path)\n",
    "if_I.model = UnetFirstStage(first_stage_unet_ir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd5641",
   "metadata": {},
   "source": [
    "# Running Dream pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ff8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER CALL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548b0b81789f4e6c8aa67027d40cec91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a803d48eb6e451ab091c5d0ed9df634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = 'a photo of hamster with sign that says \"it runs on OpenVINO\" styled as a soviet cartoon'\n",
    "style_prompt = 'in rage meme style'\n",
    "\n",
    "count = 1\n",
    "\n",
    "result = dream(\n",
    "    t5=t5, if_I=if_I, #if_II=if_I,\n",
    "    prompt=[f'{style_prompt}, {prompt}']*count,\n",
    "    seed=16,\n",
    "    if_I_kwargs={\n",
    "        \"guidance_scale\": 7.0,\n",
    "        \"sample_timestep_respacing\": \"smart100\",\n",
    "    },\n",
    "#     if_II_kwargs={\n",
    "#         \"guidance_scale\": 4.0,\n",
    "#         \"sample_timestep_respacing\": \"smart50\",\n",
    "#     },\n",
    ")\n",
    "if_I.show(result['I'], size=3)\n",
    "# if_I.show(result['II'], size=6)\n",
    "# if_I.show(result['III'], size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd692573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
