{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate creative QR codes with ControlNet QR Code Monster and OpenVINOâ„¢\n",
    "\n",
    "[Stable Diffusion](https://github.com/CompVis/stable-diffusion), a cutting-edge image generation technique, but it can be further enhanced by combining it with [ControlNet](https://arxiv.org/abs/2302.05543), a widely used control network approach. The combination allows Stable Diffusion to use a condition input to guide the image generation process, resulting in highly accurate and visually appealing images. The condition input could be in the form of various types of data such as scribbles, edge maps, pose key points, depth maps, segmentation maps, normal maps, or any other relevant information that helps to guide the content of the generated image, for example - QR codes! This method can be particularly useful in complex image generation scenarios where precise control and fine-tuning are required to achieve the desired results.\n",
    "\n",
    "In this tutorial, we will learn how to convert and run [Controlnet QR Code Monster For SD-1.5](https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster) by [monster-labs](https://qrcodemonster.art/).\n",
    "\n",
    "![](https://github.com/openvinotoolkit/openvino_notebooks/assets/76463150/1a5978c6-e7a0-4824-9318-a3d8f4912c47)\n",
    "\n",
    "If you want to learn more about ControlNet and particularly on conditioning by pose, please refer to this [tutorial](../235-controlnet-stable-diffusion/235-controlnet-stable-diffusion.ipynb)\n",
    "\n",
    "#### Table of contents:\n",
    "- [Prerequisites](#Prerequisites-Uparrow)\n",
    "- [Instantiating Generation Pipeline](#Instantiating-Generation-Pipeline-Uparrow)\n",
    "- [Convert models to OpenVINO Intermediate representation (IR) format](#Convert-models-to-OpenVINO-Intermediate-representation-(IR)-format-Uparrow)\n",
    "    - [ControlNet conversion](#ControlNet-conversion-Uparrow)\n",
    "    - [UNet conversion](#UNet-conversion-Uparrow)\n",
    "    - [Text Encoder](#Text-Encoder-Uparrow)\n",
    "    - [VAE Decoder conversion](#VAE-Decoder-conversion-Uparrow)\n",
    "- [Select inference device](#Select-inference-device-for-Stable-Diffusion-pipeline-Uparrow)\n",
    "- [Prepare Inference pipeline](#Prepare-Inference-pipeline-Uparrow)\n",
    "- [Running Text-to-Image Generation with ControlNet Conditioning and OpenVINO](#Running-Text-to-Image-Generation-with-ControlNet-Conditioning-and-OpenVINO-Uparrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q accelerate diffusers transformers torch gradio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"openvino>=2023.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating Generation Pipeline [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "### ControlNet in Diffusers library\n",
    "\n",
    "For working with Stable Diffusion and ControlNet models, we will use Hugging Face [Diffusers](https://github.com/huggingface/diffusers) library. To experiment with ControlNet, Diffusers exposes the [`StableDiffusionControlNetPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet) similar to the [other Diffusers pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview). Central to the `StableDiffusionControlNetPipeline` is the `controlnet` argument which enables providing a particularly trained [`ControlNetModel`](https://huggingface.co/docs/diffusers/main/en/api/models#diffusers.ControlNetModel) instance while keeping the pre-trained diffusion model weights the same. The code below demonstrates how to create `StableDiffusionControlNetPipeline`, using the `controlnet-openpose` controlnet model and `stable-diffusion-v1-5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59e09295816402b85e9c3f0976d2707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "`prompt` has to be of type `str` or `list` but is <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplause underwater high quality\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m negative_prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLow quality\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m audio \u001b[39m=\u001b[39m pipe(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     negative_prompt\u001b[39m=\u001b[39;49mnegative_prompt,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     num_inference_steps\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     audio_length_in_s\u001b[39m=\u001b[39;49m\u001b[39m7.0\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m )\u001b[39m.\u001b[39maudios[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m sampling_rate \u001b[39m=\u001b[39m \u001b[39m16000\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m Audio(audio, rate\u001b[39m=\u001b[39msampling_rate)\n",
      "File \u001b[0;32m~/openvino_notebooks/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/openvino_notebooks/.venv/lib/python3.10/site-packages/diffusers/pipelines/audioldm2/pipeline_audioldm2.py:855\u001b[0m, in \u001b[0;36mAudioLDM2Pipeline.__call__\u001b[0;34m(self, prompt, audio_length_in_s, num_inference_steps, guidance_scale, negative_prompt, num_waveforms_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, generated_prompt_embeds, negative_generated_prompt_embeds, attention_mask, negative_attention_mask, max_new_tokens, return_dict, callback, callback_steps, cross_attention_kwargs, output_type)\u001b[0m\n\u001b[1;32m    848\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    849\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAudio length in seconds \u001b[39m\u001b[39m{\u001b[39;00maudio_length_in_s\u001b[39m}\u001b[39;00m\u001b[39m is increased to \u001b[39m\u001b[39m{\u001b[39;00mheight\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39mvocoder_upsample_factor\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    850\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mso that it can be handled by the model. It will be cut to \u001b[39m\u001b[39m{\u001b[39;00maudio_length_in_s\u001b[39m}\u001b[39;00m\u001b[39m after the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    851\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdenoising process.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    852\u001b[0m     )\n\u001b[1;32m    854\u001b[0m \u001b[39m# 1. Check inputs. Raise error if not correct\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_inputs(\n\u001b[1;32m    856\u001b[0m     prompt,\n\u001b[1;32m    857\u001b[0m     audio_length_in_s,\n\u001b[1;32m    858\u001b[0m     vocoder_upsample_factor,\n\u001b[1;32m    859\u001b[0m     callback_steps,\n\u001b[1;32m    860\u001b[0m     negative_prompt,\n\u001b[1;32m    861\u001b[0m     prompt_embeds,\n\u001b[1;32m    862\u001b[0m     negative_prompt_embeds,\n\u001b[1;32m    863\u001b[0m     generated_prompt_embeds,\n\u001b[1;32m    864\u001b[0m     negative_generated_prompt_embeds,\n\u001b[1;32m    865\u001b[0m     attention_mask,\n\u001b[1;32m    866\u001b[0m     negative_attention_mask,\n\u001b[1;32m    867\u001b[0m )\n\u001b[1;32m    869\u001b[0m \u001b[39m# 2. Define call parameters\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[39mif\u001b[39;00m prompt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/openvino_notebooks/.venv/lib/python3.10/site-packages/diffusers/pipelines/audioldm2/pipeline_audioldm2.py:667\u001b[0m, in \u001b[0;36mAudioLDM2Pipeline.check_inputs\u001b[0;34m(self, prompt, audio_length_in_s, vocoder_upsample_factor, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, generated_prompt_embeds, negative_generated_prompt_embeds, attention_mask, negative_attention_mask)\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mProvide either `prompt`, or `prompt_embeds` and `generated_prompt_embeds`. Cannot leave \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`prompt` undefined without specifying both `prompt_embeds` and `generated_prompt_embeds`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    665\u001b[0m     )\n\u001b[1;32m    666\u001b[0m \u001b[39melif\u001b[39;00m prompt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m--> 667\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`prompt` has to be of type `str` or `list` but is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    669\u001b[0m \u001b[39mif\u001b[39;00m negative_prompt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m negative_prompt_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    671\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot forward both `negative_prompt`: \u001b[39m\u001b[39m{\u001b[39;00mnegative_prompt\u001b[39m}\u001b[39;00m\u001b[39m and `negative_prompt_embeds`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    672\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mnegative_prompt_embeds\u001b[39m}\u001b[39;00m\u001b[39m. Please make sure to only forward one of the two.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    673\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `prompt` has to be of type `str` or `list` but is <class 'int'>"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from diffusers import AudioLDM2Pipeline\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "\n",
    "import gc\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "\n",
    "MODEL_ID = \"cvssp/audioldm2\"\n",
    "pipe = AudioLDM2Pipeline.from_pretrained(MODEL_ID)\n",
    "\n",
    "prompt = \"applause underwater high quality\"\n",
    "negative_prompt = \"Low quality\"\n",
    "audio = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=3,\n",
    "    audio_length_in_s=7.0\n",
    ").audios[0]\n",
    "\n",
    "sampling_rate = 16000\n",
    "Audio(audio, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert models to OpenVINO Intermediate representation (IR) format [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "We need to provide a model object, input data for model tracing to `ov.convert_model` function to obtain OpenVINO `ov.Model` object instance. Model can be saved on disk for next deployment using `ov.save_model` function.\n",
    "\n",
    "The pipeline consists of four important parts:\n",
    "\n",
    "* ControlNet for conditioning by image annotation.\n",
    "* Text Encoder for creation condition to generate an image from a text prompt.\n",
    "* Unet for step-by-step denoising latent image representation.\n",
    "* Autoencoder (VAE) for decoding latent space to image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import openvino as ov\n",
    "import torch\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoder [$\\Uparrow$](#Table-of-content:)\n",
    "The text-encoder is responsible for transforming the input prompt, for example, \"a photo of an astronaut riding a horse\" into an embedding space that can be understood by the U-Net. It is usually a simple transformer-based encoder that maps a sequence of input tokens to a sequence of latent text embeddings.\n",
    "\n",
    "The input of the text encoder is tensor `input_ids`, which contains indexes of tokens from text processed by the tokenizer and padded to the maximum length accepted by the model. Model outputs are two tensors: `last_hidden_state` - hidden state from the last MultiHeadAttention layer in the model and `pooler_out` - pooled output for whole model hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Encoder successfully converted to IR\n"
     ]
    }
   ],
   "source": [
    "class ClapEncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        encoder.eval()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.encoder.get_text_features(input_ids)\n",
    "\n",
    "clap_text_encoder_ir_path = Path('./clap_text_encoder.xml')\n",
    "\n",
    "if not clap_text_encoder_ir_path.exists():\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(\n",
    "            ClapEncoderWrapper(pipe.text_encoder),  # model instance\n",
    "            example_input=torch.ones((1, 512), dtype=torch.long),  # inputs for model tracing\n",
    "        )\n",
    "    ov.save_model(ov_model, clap_text_encoder_ir_path)\n",
    "    # del ov_model\n",
    "    # del pipe.text_encoder\n",
    "    cleanup_torchscript_cache()\n",
    "    print('Text Encoder successfully converted to IR')\n",
    "else:\n",
    "    # del pipe.text_encoder\n",
    "    print(f\"Text Encoder will be loaded from {clap_text_encoder_ir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Encoder successfully converted to IR\n"
     ]
    }
   ],
   "source": [
    "t5_text_encoder_ir_path = Path('./t5_text_encoder.xml')\n",
    "\n",
    "if not t5_text_encoder_ir_path.exists():\n",
    "    pipe.text_encoder_2.eval()\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(\n",
    "            pipe.text_encoder_2,  # model instance\n",
    "            example_input=torch.ones((1, 7), dtype=torch.long),  # inputs for model tracing\n",
    "        )\n",
    "    ov.save_model(ov_model, t5_text_encoder_ir_path)\n",
    "    # del ov_model\n",
    "    # del pipe.text_encoder_2\n",
    "    cleanup_torchscript_cache()\n",
    "    print('Text Encoder successfully converted to IR')\n",
    "else:\n",
    "    # del pipe.text_encoder_2\n",
    "    print(f\"Text Encoder will be loaded from {t5_text_encoder_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocoder conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Vocoder successfully converted to IR\n"
     ]
    }
   ],
   "source": [
    "vocoder_ir_path = Path('./vocoder.xml')\n",
    "\n",
    "if not vocoder_ir_path.exists():\n",
    "    pipe.vocoder.eval()\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(\n",
    "            pipe.vocoder,  # model instance\n",
    "            example_input=torch.ones((1, 700, 64), dtype=torch.float32),  # inputs for model tracing\n",
    "        )\n",
    "    ov.save_model(ov_model, vocoder_ir_path)\n",
    "    # del ov_model\n",
    "    # del pipe.vocoder\n",
    "    cleanup_torchscript_cache()\n",
    "    print('The Vocoder successfully converted to IR')\n",
    "else:\n",
    "    # del pipe.vocoder\n",
    "    print(f\"The Vocoder will be loaded from {vocoder_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'language_model_ir_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m core \u001b[39m=\u001b[39m ov\u001b[39m.\u001b[39mCore()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m core\u001b[39m.\u001b[39mcompile_model(language_model_ir_path)(language_model_inputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'language_model_ir_path' is not defined"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "core.compile_model(language_model_ir_path)(language_model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'language_model_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pipe\u001b[39m.\u001b[39mlanguage_model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtorchscript \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m pipe\u001b[39m.\u001b[39mlanguage_model\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m \u001b[39m=\u001b[39m partial(pipe\u001b[39m.\u001b[39mlanguage_model\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m, kwargs\u001b[39m=\u001b[39m{\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m})\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22496c79612d31302e3138352e31362e3336227d/home/idavidyu/openvino_notebooks/notebooks/267-audiodm/267-audiodm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m pipe\u001b[39m.\u001b[39mlanguage_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlanguage_model_inputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'language_model_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "pipe.language_model.config.torchscript = False\n",
    "pipe.language_model.__call__ = partial(pipe.language_model.__call__, kwargs={\n",
    "                \"past_key_values\": None,\n",
    "                \"use_cache\": False,\n",
    "                \"return_dict\": True})\n",
    "pipe.language_model(**language_model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idavidyu/openvino_notebooks/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:801: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Projection Model successfully converted to IR\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "language_model_ir_path = Path('./language_model.xml')\n",
    "\n",
    "language_model_inputs = {\n",
    "    \"inputs_embeds\": torch.randn((1, 12, 768), dtype=torch.float32),\n",
    "    \"attention_mask\": torch.ones((1, 12), dtype=torch.int64),\n",
    "}\n",
    "\n",
    "if not language_model_ir_path.exists():\n",
    "    pipe.language_model.config.torchscript = True\n",
    "    pipe.language_model.eval()\n",
    "    pipe.language_model.__call__ = partial(pipe.language_model.__call__, kwargs={\n",
    "                \"past_key_values\": None,\n",
    "                \"use_cache\": False,\n",
    "                \"return_dict\": False})\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(\n",
    "            pipe.language_model,  # model instance\n",
    "            example_input=language_model_inputs,  # inputs for model tracing\n",
    "        )\n",
    "    ov.save_model(ov_model, language_model_ir_path)\n",
    "    # del ov_model\n",
    "    # del pipe.language_model\n",
    "    cleanup_torchscript_cache()\n",
    "    print('The Projection Model successfully converted to IR')\n",
    "else:\n",
    "    # del pipe.language_model\n",
    "    print(f\"The Projection Model will be loaded from {language_model_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection model conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Projection Model successfully converted to IR\n"
     ]
    }
   ],
   "source": [
    "projection_model_ir_path = Path('./projection_model.xml')\n",
    "\n",
    "projection_model_inputs = {\n",
    "    \"hidden_states\": torch.randn((1, 1, 512), dtype=torch.float32),\n",
    "    \"hidden_states_1\": torch.randn((1, 7, 1024), dtype=torch.float32),\n",
    "    \"attention_mask\": torch.ones((1, 1), dtype=torch.int64),\n",
    "    \"attention_mask_1\": torch.ones((1, 7), dtype=torch.int64),\n",
    "}\n",
    "\n",
    "if not projection_model_ir_path.exists():\n",
    "    pipe.projection_model.eval()\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(\n",
    "            pipe.projection_model,  # model instance\n",
    "            example_input=projection_model_inputs,  # inputs for model tracing\n",
    "        )\n",
    "    ov.save_model(ov_model, projection_model_ir_path)\n",
    "    # del ov_model\n",
    "    # del pipe.projection_model\n",
    "    cleanup_torchscript_cache()\n",
    "    print('The Projection Model successfully converted to IR')\n",
    "else:\n",
    "    # del pipe.projection_model\n",
    "    print(f\"The Projection Model will be loaded from {projection_model_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet conversion [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "The process of UNet model conversion remains the same, like for original Stable Diffusion model, but with respect to the new inputs generated by ControlNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idavidyu/openvino_notebooks/.venv/lib/python3.10/site-packages/diffusers/pipelines/audioldm2/modeling_audioldm2.py:719: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n",
      "/home/idavidyu/openvino_notebooks/.venv/lib/python3.10/site-packages/diffusers/models/resnet.py:260: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/idavidyu/openvino_notebooks/.venv/lib/python3.10/site-packages/diffusers/models/resnet.py:266: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/idavidyu/openvino_notebooks/.venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:641: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if current_length != target_length:\n",
      "/home/idavidyu/openvino_notebooks/.venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:656: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.shape[0] < batch_size * head_size:\n",
      "/home/idavidyu/openvino_notebooks/.venv/lib/python3.10/site-packages/diffusers/models/resnet.py:168: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/idavidyu/openvino_notebooks/.venv/lib/python3.10/site-packages/diffusers/models/resnet.py:181: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if hidden_states.shape[0] >= 64:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet successfully converted to IR\n"
     ]
    }
   ],
   "source": [
    "unet_ir_path = Path('./unet.xml')\n",
    "\n",
    "dtype_mapping = {\n",
    "    torch.float32: ov.Type.f32,\n",
    "    torch.float64: ov.Type.f64,\n",
    "    torch.int32: ov.Type.i32,\n",
    "    torch.int64: ov.Type.i64\n",
    "}\n",
    "\n",
    "def flattenize_inputs(inputs):\n",
    "    flatten_inputs = []\n",
    "    for input_data in inputs:\n",
    "        if input_data is None:\n",
    "            continue\n",
    "        if isinstance(input_data, (list, tuple)):\n",
    "            flatten_inputs.extend(flattenize_inputs(input_data))\n",
    "        else:\n",
    "            flatten_inputs.append(input_data)\n",
    "    return flatten_inputs\n",
    "\n",
    "\n",
    "pipe.unet.eval()\n",
    "unet_inputs = {\n",
    "    \"sample\": torch.randn((2, 8, 175, 16), dtype=torch.float32),\n",
    "    \"timestep\": torch.tensor(1, dtype=torch.int64),\n",
    "    \"encoder_hidden_states\": torch.randn((2, 8, 768), dtype=torch.float32),\n",
    "    \"encoder_hidden_states_1\": torch.randn((2, 7, 1024), dtype=torch.float32),\n",
    "    \"encoder_attention_mask_1\": torch.ones((2, 7), dtype=torch.int64),\n",
    "}\n",
    "\n",
    "if not unet_ir_path.exists():\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(pipe.unet, example_input=unet_inputs)\n",
    "\n",
    "    flatten_inputs = flattenize_inputs(unet_inputs.values())\n",
    "    for input_data, input_tensor in zip(flatten_inputs, ov_model.inputs):\n",
    "        input_tensor.get_node().set_partial_shape(ov.PartialShape(input_data.shape))\n",
    "        input_tensor.get_node().set_element_type(dtype_mapping[input_data.dtype])\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "        \n",
    "    ov.save_model(ov_model, unet_ir_path)\n",
    "    # del ov_model\n",
    "    # del pipe.unet\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()\n",
    "    print('Unet successfully converted to IR')\n",
    "else:\n",
    "    # del pipe.unet\n",
    "    print(f\"Unet will be loaded from {unet_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Decoder conversion [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "The VAE model has two parts, an encoder, and a decoder. The encoder is used to convert the image into a low-dimensional latent representation, which will serve as the input to the U-Net model. The decoder, conversely, transforms the latent representation back into an image.\n",
    "\n",
    "During latent diffusion training, the encoder is used to get the latent representations (latents) of the images for the forward diffusion process, which applies more and more noise at each step. During inference, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder. During inference, we will see that we **only need the VAE decoder**. You can find instructions on how to convert the encoder part in a stable diffusion [notebook](../225-stable-diffusion-text-to-image/225-stable-diffusion-text-to-image.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE decoder successfully converted to IR\n"
     ]
    }
   ],
   "source": [
    "vae_ir_path = Path('./vae.xml')\n",
    "\n",
    "\n",
    "class VAEDecoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        vae.eval()\n",
    "        self.vae = vae\n",
    "\n",
    "    def forward(self, latents):\n",
    "        return self.vae.decode(latents)\n",
    "\n",
    "if not vae_ir_path.exists():\n",
    "    vae_decoder = VAEDecoderWrapper(pipe.vae)\n",
    "    latents = torch.zeros((1, 8, 175, 16))\n",
    "\n",
    "    vae_decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(vae_decoder, example_input=latents)\n",
    "        ov.save_model(ov_model, vae_ir_path)\n",
    "    # del ov_model\n",
    "    # del pipe.vae\n",
    "    cleanup_torchscript_cache()\n",
    "    print('VAE decoder successfully converted to IR')\n",
    "else:\n",
    "    # del pipe.vae\n",
    "    print(f\"VAE decoder will be loaded from {vae_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select inference device for Stable Diffusion pipeline [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca62110126f4b69ad06ca6957b62835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Inference pipeline [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "The stable diffusion model takes both a latent seed and a text prompt as input. The latent seed is then used to generate random latent image representations of size $96 \\times 96$ where as the text prompt is transformed to text embeddings of size $77 \\times 768$ via CLIP's text encoder.\n",
    "\n",
    "Next, the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings. In comparison with the original stable-diffusion pipeline, latent image representation, encoder hidden states, and control condition annotation passed via ControlNet on each denoising step for obtaining middle and down blocks attention parameters, these attention blocks results additionally will be provided to the UNet model for the control generation process. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, it is recommended to use one of:\n",
    "\n",
    "- [PNDM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py)\n",
    "- [DDIM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py)\n",
    "- [K-LMS scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py)\n",
    "\n",
    "Theory on how the scheduler algorithm function works is out of scope for this notebook, but in short, you should remember that they compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\n",
    "For more information, it is recommended to look into [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)\n",
    "\n",
    "In this tutorial, instead of using Stable Diffusion's default [`PNDMScheduler`](https://huggingface.co/docs/diffusers/main/en/api/schedulers/pndm), we use [`EulerAncestralDiscreteScheduler`](https://huggingface.co/docs/diffusers/api/schedulers/euler_ancestral), recommended by authors. More information regarding schedulers can be found [here](https://huggingface.co/docs/diffusers/main/en/using-diffusers/schedulers).\n",
    "\n",
    "The *denoising* process is repeated a given number of times (by default 50) to step-by-step retrieve better latent image representations.\n",
    "Once complete, the latent image representation is decoded by the decoder part of the variational auto-encoder.\n",
    "\n",
    "Similarly to Diffusers `StableDiffusionControlNetPipeline`, we define our own `OVContrlNetStableDiffusionPipeline` inference pipeline based on OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_438166/1889049886.py:1: FutureWarning: Importing `DiffusionPipeline` or `ImagePipelineOutput` from diffusers.pipeline_utils is deprecated. Please import from diffusers.pipelines.pipeline_utils instead.\n",
      "  from diffusers.pipeline_utils import DiffusionPipeline\n"
     ]
    }
   ],
   "source": [
    "from diffusers.pipeline_utils import DiffusionPipeline\n",
    "from diffusers.schedulers import KarrasDiffusionSchedulers\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5TokenizerFast,\n",
    "    RobertaTokenizer,\n",
    "    RobertaTokenizerFast,\n",
    ")\n",
    "from typing import Union, List, Optional, Tuple\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scale_fit_to_window(dst_width:int, dst_height:int, image_width:int, image_height:int):\n",
    "    \"\"\"\n",
    "    Preprocessing helper function for calculating image size for resize with peserving original aspect ratio \n",
    "    and fitting image to specific window size\n",
    "    \n",
    "    Parameters:\n",
    "      dst_width (int): destination window width\n",
    "      dst_height (int): destination window height\n",
    "      image_width (int): source image width\n",
    "      image_height (int): source image height\n",
    "    Returns:\n",
    "      result_width (int): calculated width for resize\n",
    "      result_height (int): calculated height for resize\n",
    "    \"\"\"\n",
    "    im_scale = min(dst_height / image_height, dst_width / image_width)\n",
    "    return int(im_scale * image_width), int(im_scale * image_height)\n",
    "\n",
    "\n",
    "def preprocess(image: Image.Image):\n",
    "    \"\"\"\n",
    "    Image preprocessing function. Takes image in PIL.Image format, resizes it to keep aspect ration and fits to model input window 768x768,\n",
    "    then converts it to np.ndarray and adds padding with zeros on right or bottom side of image (depends from aspect ratio), after that\n",
    "    converts data to float32 data type and change range of values from [0, 255] to [-1, 1], finally, converts data layout from planar NHWC to NCHW.\n",
    "    The function returns preprocessed input tensor and padding size, which can be used in postprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "      image (Image.Image): input image\n",
    "    Returns:\n",
    "       image (np.ndarray): preprocessed image tensor\n",
    "       pad (Tuple[int]): pading size for each dimension for restoring image size in postprocessing\n",
    "    \"\"\"\n",
    "    src_width, src_height = image.size\n",
    "    dst_width, dst_height = scale_fit_to_window(768, 768, src_width, src_height)\n",
    "    image = image.convert(\"RGB\")\n",
    "    image = np.array(image.resize((dst_width, dst_height), resample=Image.Resampling.LANCZOS))[None, :]\n",
    "    pad_width = 768 - dst_width\n",
    "    pad_height = 768 - dst_height\n",
    "    pad = ((0, 0), (0, pad_height), (0, pad_width), (0, 0))\n",
    "    image = np.pad(image, pad, mode=\"constant\")\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = image.transpose(0, 3, 1, 2)\n",
    "    return image, pad\n",
    "\n",
    "\n",
    "def randn_tensor(\n",
    "    shape: Union[Tuple, List],\n",
    "    dtype: Optional[np.dtype] = np.float32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function for generation random values tensor with given shape and data type\n",
    "    \n",
    "    Parameters:\n",
    "      shape (Union[Tuple, List]): shape for filling random values\n",
    "      dtype (np.dtype, *optiona*, np.float32): data type for result\n",
    "    Returns:\n",
    "      latents (np.ndarray): tensor with random values with given data type and shape (usually represents noise in latent space)\n",
    "    \"\"\"\n",
    "    latents = np.random.randn(*shape).astype(dtype)\n",
    "\n",
    "    return latents\n",
    "\n",
    "\n",
    "class OVAudioLDM2Pipeline(DiffusionPipeline):\n",
    "    \"\"\"\n",
    "    OpenVINO inference pipeline for Stable Diffusion with ControlNet guidence\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        core: ov.Core,\n",
    "        tokenizer: Union[RobertaTokenizer, RobertaTokenizerFast],\n",
    "        tokenizer_2: Union[T5Tokenizer, T5TokenizerFast],\n",
    "        scheduler: KarrasDiffusionSchedulers,\n",
    "        text_encoder: ov.Model,\n",
    "        text_encoder_2: ov.Model,\n",
    "        vae_decoder: ov.Model,\n",
    "        projection_model: ov.Model,\n",
    "        language_model: ov.Model,\n",
    "        unet: ov.Model,\n",
    "        vocoder: ov.Model,\n",
    "        device:str = \"AUTO\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_2 = tokenizer_2\n",
    "        self.vae_scale_factor = 8\n",
    "        self.scheduler = scheduler\n",
    "        self.load_models(\n",
    "            core, device,\n",
    "            text_encoder, text_encoder_2,\n",
    "            unet, vae_decoder,\n",
    "            projection_model, language_model, vocoder\n",
    "        )\n",
    "        self.set_progress_bar_config(disable=True)\n",
    "\n",
    "    def load_models(\n",
    "            self, core: ov.Core, device: str,\n",
    "            text_encoder: ov.Model, text_encoder_2: ov.Model,\n",
    "            unet: ov.Model, vae_decoder: ov.Model,\n",
    "            projection_model: ov.Model, language_model: ov.Model,\n",
    "            vocoder: ov.Model\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Function for loading models on device using OpenVINO\n",
    "        \n",
    "        Parameters:\n",
    "          core (Core): OpenVINO runtime Core class instance\n",
    "          device (str): inference device\n",
    "          controlnet (Model): OpenVINO Model object represents ControlNet\n",
    "          text_encoder (Model): OpenVINO Model object represents text encoder\n",
    "          unet (Model): OpenVINO Model object represents UNet\n",
    "          vae_decoder (Model): OpenVINO Model object represents vae decoder\n",
    "        Returns\n",
    "          None\n",
    "        \"\"\"\n",
    "        self.text_encoder = core.compile_model(text_encoder, device)\n",
    "        self.text_encoder_out = self.text_encoder.output(0)\n",
    "        self.text_encoder_2 = core.compile_model(text_encoder_2, device)\n",
    "        self.text_encoder_2_out = self.text_encoder_2.output(0)\n",
    "        self.unet = core.compile_model(unet, device)\n",
    "        self.unet_out = self.unet.output(0)\n",
    "        self.vae_decoder = core.compile_model(vae_decoder, device)\n",
    "        self.vae_decoder_out = self.vae_decoder.output(0)\n",
    "        self.projection_model = core.compile_model(projection_model, device)\n",
    "        self.projection_model_out = self.projection_model.output(0)\n",
    "        self.language_model = core.compile_model(language_model, device)\n",
    "        self.language_model_out = self.language_model.output(0)\n",
    "        self.vocoder = core.compile_model(vocoder, device)\n",
    "        self.vocoder_out = self.vocoder.output(0)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        audio_length_in_s: Optional[float] = None,\n",
    "        num_inference_steps: int = 50,\n",
    "        guidance_scale: float = 7.5,\n",
    "        negative_prompt: Union[str, List[str]] = None,\n",
    "        eta: float = 0.0,\n",
    "        latents: Optional[np.array] = None,\n",
    "        output_type: Optional[str] = \"np\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Parameters:\n",
    "            prompt (`str` or `List[str]`):\n",
    "                The prompt or prompts to guide the image generation.\n",
    "            image (`Image.Image`):\n",
    "                `Image`, or tensor representing an image batch which will be repainted according to `prompt`.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 100):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            negative_prompt (`str` or `List[str]`):\n",
    "                negative prompt or prompts for generation\n",
    "            guidance_scale (`float`, *optional*, defaults to 7.5):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality. This pipeline requires a value of at least `1`.\n",
    "            latents (`np.ndarray`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `Image.Image` or `np.array`.\n",
    "        Returns:\n",
    "            image ([List[Union[np.ndarray, Image.Image]]): generaited images\n",
    "            \n",
    "        \"\"\"\n",
    "        # 0. Convert audio input length from seconds to spectrogram height\n",
    "        vocoder_upsample_factor = 0.01\n",
    "        if audio_length_in_s is None:\n",
    "            audio_length_in_s = 5\n",
    "\n",
    "        height = int(audio_length_in_s / vocoder_upsample_factor)\n",
    "\n",
    "        original_waveform_length = int(audio_length_in_s * sampling_rate)\n",
    "        if height % 4 != 0:\n",
    "            height = int(np.ceil(height / self.vae_scale_factor)) * 4\n",
    "            print(\n",
    "                f\"Audio length in seconds {audio_length_in_s} is increased to {height * vocoder_upsample_factor} \"\n",
    "                f\"so that it can be handled by the model. It will be cut to {audio_length_in_s} after the \"\n",
    "                f\"denoising process.\"\n",
    "            )\n",
    "\n",
    "        # 1. Define call parameters\n",
    "        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        # 2. Encode input prompt\n",
    "        text_embeddings = self._encode_prompt(prompt, negative_prompt=negative_prompt)\n",
    "\n",
    "        # 3. Preprocess image\n",
    "        orig_width, orig_height = image.size\n",
    "        image, pad = preprocess(image)\n",
    "        height, width = image.shape[-2:]\n",
    "        if do_classifier_free_guidance:\n",
    "            image = np.concatenate(([image] * 2))\n",
    "\n",
    "        # 4. set timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        # 6. Prepare latent variables\n",
    "        num_channels_latents = 4\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            text_embeddings.dtype,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # Expand the latents if we are doing classifier free guidance.\n",
    "                # The latents are expanded 3 times because for pix2pix the guidance\\\n",
    "                # is applied for both the text and the input image.\n",
    "                latent_model_input = np.concatenate(\n",
    "                    [latents] * 2) if do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                result = self.controlnet([latent_model_input, t, text_embeddings, image])\n",
    "                down_and_mid_blok_samples = [sample * controlnet_conditioning_scale for _, sample in result.items()]\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = self.unet([latent_model_input, t, text_embeddings, *down_and_mid_blok_samples])[self.unet_out]\n",
    "\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred[0], noise_pred[1]\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(torch.from_numpy(noise_pred), t, torch.from_numpy(latents)).prev_sample.numpy()\n",
    "\n",
    "                # update progress\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "\n",
    "        # 8. Post-processing\n",
    "        image = self.decode_latents(latents, pad)\n",
    "\n",
    "        # 9. Convert to PIL\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "            image = [img.resize((orig_width, orig_height), Image.Resampling.LANCZOS) for img in image]\n",
    "        else:\n",
    "            image = [cv2.resize(img, (orig_width, orig_width))\n",
    "                     for img in image]\n",
    "\n",
    "        return image\n",
    "\n",
    "    def _encode_prompt(self, prompt:Union[str, List[str]], num_images_per_prompt:int = 1, do_classifier_free_guidance:bool = True, negative_prompt:Union[str, List[str]] = None):\n",
    "        \"\"\"\n",
    "        Encodes the prompt into text encoder hidden states.\n",
    "\n",
    "        Parameters:\n",
    "            prompt (str or list(str)): prompt to be encoded\n",
    "            num_images_per_prompt (int): number of images that should be generated per prompt\n",
    "            do_classifier_free_guidance (bool): whether to use classifier free guidance or not\n",
    "            negative_prompt (str or list(str)): negative prompt to be encoded\n",
    "        Returns:\n",
    "            text_embeddings (np.ndarray): text encoder hidden states\n",
    "        \"\"\"\n",
    "        batch_size = len(prompt) if isinstance(prompt, list) else 1\n",
    "\n",
    "        tokenizers = [self.tokenizer, self.tokenizer_2]\n",
    "        text_encoders = [self.text_encoder, self.text_encoder_2]\n",
    "\n",
    "        prompt_embeds_list = []\n",
    "        attention_mask_list = []\n",
    "\n",
    "        for i, tokenizer, text_encoder in enumerate(zip(tokenizers, text_encoders)):\n",
    "            text_inputs = tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\" if isinstance(tokenizer, (RobertaTokenizer, RobertaTokenizerFast)) else True,\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"np\",\n",
    "            )\n",
    "            text_input_ids = text_inputs.input_ids\n",
    "            attention_mask = text_inputs.attention_mask\n",
    "\n",
    "            if i == 0:\n",
    "                # CLAP encoder\n",
    "                prompt_embeds = text_encoder(\n",
    "            t       ext_input_ids)[self.text_encoder_out]\n",
    "                # append the seq-len dim: (bs, hidden_size) -> (bs, seq_len, hidden_size)\n",
    "                prompt_embeds = prompt_embeds[:, None, :]\n",
    "                # # make sure that we attend to this single hidden-state\n",
    "                # attention_mask = attention_mask.new_ones((batch_size, 1))\n",
    "            else:\n",
    "                # t5 encoder\n",
    "                prompt_embeds = text_encoder(\n",
    "                    text_input_ids)[self.text_encoder_2_out]\n",
    "                prompt_embeds = prompt_embeds[0]\n",
    "\n",
    "            prompt_embeds_list.append(prompt_embeds)\n",
    "            attention_mask_list.append(attention_mask)\n",
    "\n",
    "        # Projection model\n",
    "        projection_output = self.projection_model(\n",
    "            prompt_embeds_list[0],\n",
    "            prompt_embeds_list[1],\n",
    "            attention_mask_list[0],\n",
    "            attention_mask_list[1],\n",
    "        )\n",
    "        projected_prompt_embeds = projection_output[0]\n",
    "        projected_attention_mask = projection_output[1]\n",
    "\n",
    "        generated_prompt_embeds = self.generate_language_model(\n",
    "            projected_prompt_embeds,\n",
    "            attention_mask=projected_attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "\n",
    "        # duplicate text embeddings for each generation per prompt\n",
    "        if num_images_per_prompt != 1:\n",
    "            bs_embed, seq_len, _ = text_embeddings.shape\n",
    "            text_embeddings = np.tile(\n",
    "                text_embeddings, (1, num_images_per_prompt, 1))\n",
    "            text_embeddings = np.reshape(\n",
    "                text_embeddings, (bs_embed * num_images_per_prompt, seq_len, -1))\n",
    "\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            uncond_tokens: List[str]\n",
    "            max_length = text_input_ids.shape[-1]\n",
    "            if negative_prompt is None:\n",
    "                uncond_tokens = [\"\"] * batch_size\n",
    "            elif isinstance(negative_prompt, str):\n",
    "                uncond_tokens = [negative_prompt]\n",
    "            else:\n",
    "                uncond_tokens = negative_prompt\n",
    "            uncond_input = self.tokenizer(\n",
    "                uncond_tokens,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"np\",\n",
    "            )\n",
    "\n",
    "            uncond_embeddings = self.text_encoder(uncond_input.input_ids)[self.text_encoder_out]\n",
    "\n",
    "            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "            seq_len = uncond_embeddings.shape[1]\n",
    "            uncond_embeddings = np.tile(uncond_embeddings, (1, num_images_per_prompt, 1))\n",
    "            uncond_embeddings = np.reshape(uncond_embeddings, (batch_size * num_images_per_prompt, seq_len, -1))\n",
    "\n",
    "            # For classifier free guidance, we need to do two forward passes.\n",
    "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "            # to avoid doing two forward passes\n",
    "            text_embeddings = np.concatenate([uncond_embeddings, text_embeddings])\n",
    "\n",
    "        return text_embeddings\n",
    "\n",
    "    def generate_language_model(\n",
    "        self,\n",
    "        inputs_embeds: np.ndarray,\n",
    "        max_new_tokens: int = 8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Generates a sequence of hidden-states from the language model, conditioned on the embedding inputs.\n",
    "\n",
    "        Parameters:\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                The sequence used as a prompt for the generation.\n",
    "            max_new_tokens (`int`):\n",
    "                Number of new tokens to generate.\n",
    "            model_kwargs (`Dict[str, Any]`, *optional*):\n",
    "                Ad hoc parametrization of additional model-specific kwargs that will be forwarded to the `forward`\n",
    "                function of the model.\n",
    "\n",
    "        Return:\n",
    "            `inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "                The sequence of generated hidden-states.\n",
    "        \"\"\"\n",
    "        max_new_tokens = max_new_tokens if max_new_tokens is not None else self.language_model.config.max_new_tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # prepare model inputs\n",
    "            model_inputs = prepare_inputs_for_generation(inputs_embeds, **model_kwargs)\n",
    "\n",
    "            # forward pass to get next hidden states\n",
    "            output = self.language_model(**model_inputs, return_dict=True)\n",
    "\n",
    "            next_hidden_states = output.last_hidden_state\n",
    "\n",
    "            # Update the model input\n",
    "            inputs_embeds = torch.cat([inputs_embeds, next_hidden_states[:, -1:, :]], dim=1)\n",
    "\n",
    "            # Update generated hidden states, model inputs, and length for next step\n",
    "            model_kwargs = self.language_model._update_model_kwargs_for_generation(output, model_kwargs)\n",
    "\n",
    "        return inputs_embeds[:, -max_new_tokens:, :]\n",
    "\n",
    "    def prepare_latents(self, batch_size:int, num_channels_latents:int, height:int, width:int, dtype:np.dtype = np.float32, latents:np.ndarray = None):\n",
    "        \"\"\"\n",
    "        Preparing noise to image generation. If initial latents are not provided, they will be generated randomly, \n",
    "        then prepared latents scaled by the standard deviation required by the scheduler\n",
    "        \n",
    "        Parameters:\n",
    "           batch_size (int): input batch size\n",
    "           num_channels_latents (int): number of channels for noise generation\n",
    "           height (int): image height\n",
    "           width (int): image width\n",
    "           dtype (np.dtype, *optional*, np.float32): dtype for latents generation\n",
    "           latents (np.ndarray, *optional*, None): initial latent noise tensor, if not provided will be generated\n",
    "        Returns:\n",
    "           latents (np.ndarray): scaled initial noise for diffusion\n",
    "        \"\"\"\n",
    "        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n",
    "        if latents is None:\n",
    "            latents = randn_tensor(shape, dtype=dtype)\n",
    "        else:\n",
    "            latents = latents\n",
    "\n",
    "        # scale the initial noise by the standard deviation required by the scheduler\n",
    "        latents = latents * np.array(self.scheduler.init_noise_sigma)\n",
    "        return latents\n",
    "\n",
    "    def decode_latents(self, latents:np.array, pad:Tuple[int]):\n",
    "        \"\"\"\n",
    "        Decode predicted image from latent space using VAE Decoder and unpad image result\n",
    "        \n",
    "        Parameters:\n",
    "           latents (np.ndarray): image encoded in diffusion latent space\n",
    "           pad (Tuple[int]): each side padding sizes obtained on preprocessing step\n",
    "        Returns:\n",
    "           image: decoded by VAE decoder image\n",
    "        \"\"\"\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = self.vae_decoder(latents)[self.vae_decoder_out]\n",
    "        (_, end_h), (_, end_w) = pad[1:3]\n",
    "        h, w = image.shape[2:]\n",
    "        unpad_h = h - end_h\n",
    "        unpad_w = w - end_w\n",
    "        image = image[:, :, :unpad_h, :unpad_w]\n",
    "        image = np.clip(image / 2 + 0.5, 0, 1)\n",
    "        image = np.transpose(image, (0, 2, 3, 1))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Text-to-Image Generation with ControlNet Conditioning and OpenVINO [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "Now, we are ready to start generation. For improving the generation process, we also introduce an opportunity to provide a `negative prompt`. Technically, positive prompt steers the diffusion toward the images associated with it, while negative prompt steers the diffusion away from it. More explanation of how it works can be found in this [article](https://stable-diffusion-art.com/how-negative-prompt-work/). We can keep this field empty if we want to generate image without negative prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import EulerAncestralDiscreteScheduler\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
    "scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "ov_pipe = OVContrlNetStableDiffusionPipeline(tokenizer, scheduler, core, controlnet_ir_path, text_encoder_ir_path, unet_ir_path, vae_ir_path, device=device.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qrcode\n",
    "\n",
    "def create_code(content: str):\n",
    "    \"\"\"Creates QR codes with provided content.\"\"\"\n",
    "    qr = qrcode.QRCode(\n",
    "        version=1,\n",
    "        error_correction=qrcode.constants.ERROR_CORRECT_H,\n",
    "        box_size=16,\n",
    "        border=0,\n",
    "    )\n",
    "    qr.add_data(content)\n",
    "    qr.make(fit=True)\n",
    "    img = qr.make_image(fill_color=\"black\", back_color=\"white\")\n",
    "\n",
    "    # find smallest image size multiple of 256 that can fit qr\n",
    "    offset_min = 8 * 16\n",
    "    w, h = img.size\n",
    "    w = (w + 255 + offset_min) // 256 * 256\n",
    "    h = (h + 255 + offset_min) // 256 * 256\n",
    "    if w > 1024:\n",
    "        raise gr.Error(\"QR code is too large, please use a shorter content\")\n",
    "    bg = Image.new('L', (w, h), 128)\n",
    "\n",
    "    # align on 16px grid\n",
    "    coords = ((w - img.size[0]) // 2 // 16 * 16,\n",
    "              (h - img.size[1]) // 2 // 16 * 16)\n",
    "    bg.paste(img, coords)\n",
    "    return bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def _generate(\n",
    "    qr_code_content: str,\n",
    "    prompt: str,\n",
    "    negative_prompt: str,\n",
    "    seed: Optional[int] = 42,\n",
    "    guidance_scale: float = 10.0,\n",
    "    controlnet_conditioning_scale: float = 2.0,\n",
    "    num_inference_steps: int = 5,\n",
    "):\n",
    "    if seed is not None:\n",
    "        np.random.seed(int(seed))\n",
    "    qrcode_image = create_code(qr_code_content)\n",
    "    return ov_pipe(\n",
    "        prompt, qrcode_image, negative_prompt=negative_prompt,\n",
    "        num_inference_steps=int(num_inference_steps),\n",
    "        guidance_scale=guidance_scale,\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale\n",
    "    )[0]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    _generate,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"QR Code content\"),\n",
    "        gr.Textbox(label=\"Text Prompt\"),\n",
    "        gr.Textbox(label=\"Negative Text Prompt\"),\n",
    "        gr.Number(\n",
    "            minimum=-1,\n",
    "            maximum=9999999999,\n",
    "            step=1,\n",
    "            value=42,\n",
    "            label=\"Seed\",\n",
    "            info=\"Seed for the random number generator\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.0,\n",
    "            maximum=25.0,\n",
    "            step=0.25,\n",
    "            value=7,\n",
    "            label=\"Guidance Scale\",\n",
    "            info=\"Controls the amount of guidance the text prompt guides the image generation\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.5,\n",
    "            maximum=2.5,\n",
    "            step=0.01,\n",
    "            value=1.5,\n",
    "            label=\"Controlnet Conditioning Scale\",\n",
    "            info=\"\"\"Controls the readability/creativity of the QR code.\n",
    "            High values: The generated QR code will be more readable.\n",
    "            Low values: The generated QR code will be more creative.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        gr.Slider(label=\"Steps\", step=1, value=5, minimum=1, maximum=50)\n",
    "    ],\n",
    "    outputs=[\n",
    "        \"image\"\n",
    "    ],\n",
    "    examples=[\n",
    "        [\n",
    "            \"Hi OpenVINO\",\n",
    "            \"snowy mountains 8k\",\n",
    "            \"blurry unreal occluded\",\n",
    "            42, 7, 1.7, 5\n",
    "        ],\n",
    "    ],\n",
    ")\n",
    "try:\n",
    "    demo.queue().launch(debug=True)\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True, debug=True)\n",
    "\n",
    "# If you are launching remotely, specify server_name and server_port\n",
    "# EXAMPLE: `demo.launch(server_name='your server name', server_port='server port in int')`\n",
    "# To learn more please refer to the Gradio docs: https://gradio.app/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
